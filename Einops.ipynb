{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI4rHtWJKTMXdiEABBAv2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vulcan-332/mini_einops/blob/main/Einops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Pattern Parser"
      ],
      "metadata": {
        "id": "s05oMA-ERSDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "def parse_pattern(pattern: str) -> Tuple[List[str], List[str]]:\n",
        "  if '->' not in pattern:\n",
        "    raise ValueError(\"Pattern must contain '->' to separate left and right parts.\")\n",
        "  left_part, right_part = pattern.split('->') # use the split function to find the '->' symbols and split the string in left and right\n",
        "  left_part = left_part.strip()\n",
        "  right_part = right_part.strip()\n",
        "\n",
        "  left_tokens = _tokenize(left_part)\n",
        "  right_tokens = _tokenize(right_part)\n",
        "\n",
        "  return left_tokens, right_tokens\n",
        "\n",
        "def _tokenize(part: str) -> List[str]:\n",
        "  pattern = r'\\([^)]*\\)|\\S+'\n",
        "  return re.findall(pattern, part)\n",
        "parse_pattern('(h w) c -> h w c')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs4Vu-nfPTPB",
        "outputId": "ce929bd1-613a-418b-ffa2-ead332e91380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['(h w)', 'c'], ['h', 'w', 'c'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Side"
      ],
      "metadata": {
        "id": "faxbw3AkcuWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "\n",
        "def infer_left_axes(left_tokens: List[str], tensor: np.ndarray, axes_lengths: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Given the left tokens (e.g. [\"b\", \"(c h)\", \"...\", \"w\"]) and tensor of shape (2,3,4,5,6),\n",
        "    and possibly user-specified axes_lengths,\n",
        "    return a fully expanded list of dimension sizes (e.g. [2, 3, 4, 5, 6]) in the correct order.\n",
        "\n",
        "    For merges like \"(c h)\" we handle them if needed (in a minimal approach,\n",
        "    you might handle them after reshaping; but let's show one way).\n",
        "    We'll do partial logic here; you may refine in your final version.\n",
        "    \"\"\"\n",
        "    shape = tensor.shape\n",
        "    n_dims = len(shape)\n",
        "    dim_ptr = 0  # pointer into shape\n",
        "\n",
        "    # We'll accumulate each dimension's size in a list:\n",
        "    final_expanded_dims = []\n",
        "\n",
        "    for token in left_tokens:\n",
        "        if token == '...':\n",
        "            # The ellipsis means \"take however many dimensions remain\" => potentially multiple dims\n",
        "            # We take all leftover dims from shape\n",
        "            leftover = shape[dim_ptr:]\n",
        "            final_expanded_dims.extend(leftover)\n",
        "            dim_ptr = n_dims  # we've used up all dims\n",
        "        elif token.startswith('(') and token.endswith(')'):\n",
        "            # Merged/split pattern like \"(c h)\"\n",
        "            # we interpret this as a single dimension from shape[dim_ptr], and try to split it.\n",
        "            inside = token[1:-1].strip()  # \"c h\"\n",
        "            subdims = inside.split()      # [\"c\", \"h\"]\n",
        "\n",
        "            # The dimension we have is shape[dim_ptr]\n",
        "            merged_dim_size = shape[dim_ptr]\n",
        "            dim_ptr += 1\n",
        "\n",
        "            # We'll distribute that dimension among subdims c and h.\n",
        "            # Suppose c=3, h=4 => c*h=12 => must match merged_dim_size.\n",
        "            # If user provided c in axes_lengths, we infer h, etc.\n",
        "            known_product = 1\n",
        "            unknown_subdims = []\n",
        "            for sd in subdims:\n",
        "                if sd in axes_lengths:\n",
        "                    known_product *= axes_lengths[sd]\n",
        "                else:\n",
        "                    unknown_subdims.append(sd)\n",
        "\n",
        "            leftover = merged_dim_size // known_product\n",
        "            if len(unknown_subdims) > 1:\n",
        "                # If more than one is unknown, we need a more advanced solver. For now we keep it simple.\n",
        "                raise ValueError(\"Too many unknown dims in the merged group. Provide enough axes_lengths.\")\n",
        "\n",
        "            if len(unknown_subdims) == 1:\n",
        "                # That subdim = leftover\n",
        "                axes_lengths[unknown_subdims[0]] = leftover\n",
        "\n",
        "            # Now we know each subdim's size:\n",
        "            for sd in subdims:\n",
        "                final_expanded_dims.append(axes_lengths[sd])\n",
        "\n",
        "        else:\n",
        "            # A normal axis name like 'b'\n",
        "            if dim_ptr >= n_dims:\n",
        "                raise ValueError(f\"Pattern expects more dims than input shape provides. Problem with token '{token}'\")\n",
        "\n",
        "            current_dim_size = shape[dim_ptr]\n",
        "            dim_ptr += 1\n",
        "\n",
        "            # Check if user specified a constraint for this axis name\n",
        "            if token in axes_lengths:\n",
        "                if axes_lengths[token] != current_dim_size:\n",
        "                    raise ValueError(\n",
        "                        f\"Mismatch for axis '{token}': pattern expects {axes_lengths[token]}, but tensor has {current_dim_size}.\"\n",
        "                    )\n",
        "            else:\n",
        "                axes_lengths[token] = current_dim_size\n",
        "\n",
        "            final_expanded_dims.append(current_dim_size)\n",
        "\n",
        "    if dim_ptr < n_dims:\n",
        "        # leftover dims not covered by pattern => error or assume user forgot ellipsis\n",
        "        raise ValueError(f\"Not all dimensions in input tensor are covered by the left pattern. \"\n",
        "                         f\"Unused dims start at index {dim_ptr}\")\n",
        "\n",
        "    return final_expanded_dims\n"
      ],
      "metadata": {
        "id": "1xuRS-BxXBO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output side"
      ],
      "metadata": {
        "id": "6_c51UGKcrG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place holder function. Actual functionality later defined in rearrange() later on.\n",
        "\n",
        "def infer_right_axes(\n",
        "    right_tokens: List[str],\n",
        "    axes_lengths: Dict[str, int],\n",
        "    expanded_input_ndims: int\n",
        "):\n",
        "    \"\"\"\n",
        "    From the right tokens (e.g. [\"b\", \"...\", \"(c h)\", \"w\"]) figure out:\n",
        "      - The order in which to read existing axes (transposition).\n",
        "      - Which merges are needed.\n",
        "      - If we have repeating or splitting instructions.\n",
        "    We'll return a structure describing final output dims and a permutation of the input dims.\n",
        "    \"\"\"\n",
        "    # Pseudocode approach:\n",
        "    # 1. We'll build a list of \"output instructions\" that say how to form each new axis.\n",
        "    # 2. If we see an ellipsis, that means \"take some number of axes in the same order they appear\".\n",
        "    # 3. If we see parenthesis \"(c h)\", that means \"merge c and h\" or \"split\" depending on context.\n",
        "    #\n",
        "\n",
        "    reorder_map = []\n",
        "    merges = []\n",
        "    repeats = []\n",
        "\n",
        "    # We'll assume for now that each named token on the right corresponds to an existing axis from left\n",
        "    # in the same order. A real solution would need a better mapping from names -> indices.\n",
        "\n",
        "    current_in_axis = 0\n",
        "    # ...\n",
        "    # TOTALLY simplified approach here, as a placeholder\n",
        "    return reorder_map, merges, repeats\n"
      ],
      "metadata": {
        "id": "wkHBzExNcqfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rearrange function\n"
      ],
      "metadata": {
        "id": "1adr846Rcyj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "import numpy as np\n",
        "\n",
        "def rearrange(tensor: np.ndarray, pattern: str, **axes_lengths: Any) -> np.ndarray:\n",
        "    left_tokens, right_tokens = parse_pattern(pattern)\n",
        "    left_expanded = infer_left_axes(left_tokens, tensor, axes_lengths)\n",
        "    tensor = tensor.reshape(*left_expanded)\n",
        "\n",
        "    # Flatten left side into axis names\n",
        "    input_axes = []\n",
        "    for token in left_tokens:\n",
        "        if token.startswith('(') and token.endswith(')'):\n",
        "            input_axes.extend(token[1:-1].split())\n",
        "        elif token == '...':\n",
        "            raise NotImplementedError(\"Ellipsis not yet supported\")\n",
        "        else:\n",
        "            input_axes.append(token)\n",
        "\n",
        "    axis_sizes = [axes_lengths[name] for name in input_axes]\n",
        "    axis_name_to_idx = {name: i for i, name in enumerate(input_axes)}\n",
        "\n",
        "    # Flatten right side into axis names\n",
        "    flat_right_names = []\n",
        "    for token in right_tokens:\n",
        "        if token.startswith('(') and token.endswith(')'):\n",
        "            flat_right_names.extend(token[1:-1].split())\n",
        "        elif token == '...':\n",
        "            raise NotImplementedError(\"Ellipsis not yet supported\")\n",
        "        else:\n",
        "            flat_right_names.append(token)\n",
        "\n",
        "    # Transpose to match flat_right_names order\n",
        "    perm = [input_axes.index(name) for name in flat_right_names if name in input_axes]\n",
        "    tensor = np.transpose(tensor, perm)\n",
        "\n",
        "    # Build shape with 1s for any new axes (e.g. for repeat)\n",
        "    final_shape = []\n",
        "    for token in right_tokens:\n",
        "        if token.startswith('(') and token.endswith(')'):\n",
        "            names = token[1:-1].split()\n",
        "            size = 1\n",
        "            for name in names:\n",
        "                if name in input_axes:\n",
        "                    size *= axes_lengths[name]\n",
        "                elif name in axes_lengths:\n",
        "                    size *= 1  # we insert a dummy for now\n",
        "            final_shape.append(size)\n",
        "        elif token in axes_lengths and token not in input_axes:\n",
        "            final_shape.append(1)  # placeholder for repeat\n",
        "        else:\n",
        "            final_shape.append(axes_lengths[token])\n",
        "\n",
        "    tensor = tensor.reshape(*final_shape)\n",
        "\n",
        "    # Apply repeat to newly introduced axes\n",
        "    for axis, token in enumerate(right_tokens):\n",
        "        if token in axes_lengths and token not in input_axes:\n",
        "            tensor = np.repeat(tensor, repeats=axes_lengths[token], axis=axis)\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "k0uCbq1Ccx4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unit Test"
      ],
      "metadata": {
        "id": "PmDj7LtoRO6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestRearrange(unittest.TestCase):\n",
        "\n",
        "  def test_basic_transpose(self):\n",
        "    x = np.random.randn(2, 3, 4)\n",
        "    y = rearrange(x, \"b c h -> h c b\")\n",
        "    self.assertEqual(y.shape, (4, 3, 2))\n",
        "\n",
        "  def test_transpose_middle(self):\n",
        "      x = np.random.randn(2, 3, 4)\n",
        "      y = rearrange(x, \"b c h -> b h c\")\n",
        "      self.assertEqual(y.shape, (2, 4, 3))\n",
        "\n",
        "  def test_merge_dims(self):\n",
        "      x = np.random.randn(2, 3, 4)\n",
        "      y = rearrange(x, \"b c h -> b (c h)\")\n",
        "      self.assertEqual(y.shape, (2, 12))\n",
        "\n",
        "  def test_split_dims(self):\n",
        "      x = np.random.randn(2, 12, 3)\n",
        "      y = rearrange(x, \"b (c h) w -> b c h w\", c=4)\n",
        "      self.assertEqual(y.shape, (2, 4, 3, 3))\n",
        "\n",
        "  def test_repeat_axis(self):\n",
        "      x = np.random.randn(2, 3)\n",
        "      y = rearrange(x, \"b c -> b c r\", r=2)\n",
        "      self.assertEqual(y.shape, (2, 3, 2))\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestRearrange)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG6Y__bYfVCM",
        "outputId": "35d98587-70fd-40e2-8c90-7543a80dc92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_basic_transpose (__main__.TestRearrange.test_basic_transpose) ... ok\n",
            "test_merge_dims (__main__.TestRearrange.test_merge_dims) ... ok\n",
            "test_repeat_axis (__main__.TestRearrange.test_repeat_axis) ... ok\n",
            "test_split_dims (__main__.TestRearrange.test_split_dims) ... ok\n",
            "test_transpose_middle (__main__.TestRearrange.test_transpose_middle) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.011s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}